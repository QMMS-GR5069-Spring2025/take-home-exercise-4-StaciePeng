{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5a6fcc3-d2e4-467b-b92e-25b477d3dc5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install mlflow mysql-connector-python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d8b17b2-ede9-4af9-8398-b63d4d41b626",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, monotonically_increasing_id\n",
    "from pyspark.sql.types import IntegerType, DoubleType\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "from pyspark.ml.regression import LinearRegression, RandomForestRegressor\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import mysql.connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f55b5273-f9d4-4fbe-827a-acf7af861c78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "lap_times = spark.read.csv(\"s3://columbia-gr5069-main/raw/lap_times.csv\", header=True, inferSchema=True)\n",
    "pit_stops = spark.read.csv(\"s3://columbia-gr5069-main/raw/pit_stops.csv\", header=True, inferSchema=True)\n",
    "drivers = spark.read.csv(\"s3://columbia-gr5069-main/raw/drivers.csv\", header=True, inferSchema=True)\n",
    "# Prepare dataset\n",
    "pit_grouped = pit_stops.groupBy(\"raceId\", \"driverId\").agg(count(\"*\").alias(\"pit_stop_count\"))\n",
    "laps_joined = lap_times.join(pit_grouped, on=[\"raceId\", \"driverId\"], how=\"left\").fillna(0)\n",
    "\n",
    "laps_features = laps_joined.select(\n",
    "    col(\"milliseconds\").cast(DoubleType()),\n",
    "    col(\"lap\").cast(IntegerType()),\n",
    "    col(\"position\").cast(IntegerType()),\n",
    "    col(\"pit_stop_count\").cast(IntegerType())\n",
    ").na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97087e95-5774-4ec3-8af7-a5d5db42a94e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create database and tables\n",
    "conn = mysql.connector.connect(\n",
    "    host=\"yp2728-gr5069.ccqalx6jsr2n.us-east-1.rds.amazonaws.com\",\n",
    "    user=\"admin\",\n",
    "    password=\"PYMpym0919!\"\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"CREATE DATABASE IF NOT EXISTS gr5069\")\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0723b93-5845-4898-9f8b-2873019a4380",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "lap_times = spark.read.csv(\"s3://columbia-gr5069-main/raw/lap_times.csv\", header=True, inferSchema=True)\n",
    "pit_stops = spark.read.csv(\"s3://columbia-gr5069-main/raw/pit_stops.csv\", header=True, inferSchema=True)\n",
    "drivers = spark.read.csv(\"s3://columbia-gr5069-main/raw/drivers.csv\", header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3be1e2b6-88df-425a-b66f-e6478b358679",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Prepare dataset\n",
    "pit_grouped = pit_stops.groupBy(\"raceId\", \"driverId\").agg(count(\"*\").alias(\"pit_stop_count\"))\n",
    "laps_joined = lap_times.join(pit_grouped, on=[\"raceId\", \"driverId\"], how=\"left\").fillna(0)\n",
    "\n",
    "laps_features = laps_joined.select(\n",
    "    col(\"milliseconds\").cast(DoubleType()),\n",
    "    col(\"lap\").cast(IntegerType()),\n",
    "    col(\"position\").cast(IntegerType()),\n",
    "    col(\"pit_stop_count\").cast(IntegerType())\n",
    ").na.drop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22506742-c821-42bd-a63b-58861148beb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Assemble features\n",
    "vec_assembler = VectorAssembler(inputCols=[\"lap\", \"position\", \"pit_stop_count\"], outputCol=\"features\")\n",
    "laps_vector = vec_assembler.transform(laps_features)\n",
    "\n",
    "# Split data\n",
    "trainDF, testDF = laps_vector.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Create database and tables\n",
    "conn = mysql.connector.connect(\n",
    "    host=\"yp2728-gr5069.ccqalx6jsr2n.us-east-1.rds.amazonaws.com\",\n",
    "    user=\"admin\",\n",
    "    password=\"PYMpym0919!\"\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"CREATE DATABASE IF NOT EXISTS gr5069\")\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18942b8b-f46b-4f0b-bf39-9c865dc8a70f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Train Linear Regression Model\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"milliseconds\")\n",
    "lrModel = lr.fit(trainDF)\n",
    "predDF_lr = lrModel.transform(testDF)\n",
    "\n",
    "# Save predictions to MySQL\n",
    "predDF_lr = predDF_lr.withColumn(\"id\", monotonically_increasing_id()).select(\"id\", \"prediction\")\n",
    "predDF_lr.write.format(\"jdbc\").options(\n",
    "    url=\"jdbc:mysql://yp2728-gr5069.ccqalx6jsr2n.us-east-1.rds.amazonaws.com/gr5069\",\n",
    "    driver=\"com.mysql.jdbc.Driver\",\n",
    "    dbtable=\"lr_predictions_f1\",\n",
    "    user=\"admin\",\n",
    "    password=\"PYMpym0919!\"\n",
    ").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee8e5769-41b2-4980-ad40-0c6d25bc762b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Train Random Forest Model\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"milliseconds\", numTrees=100)\n",
    "rfModel = rf.fit(trainDF)\n",
    "predDF_rf = rfModel.transform(testDF)\n",
    "\n",
    "# Save predictions to MySQL\n",
    "predDF_rf = predDF_rf.withColumn(\"id\", monotonically_increasing_id()).select(\"id\", \"prediction\")\n",
    "predDF_rf.write.format(\"jdbc\").options(\n",
    "    url=\"jdbc:mysql://yp2728-gr5069.ccqalx6jsr2n.us-east-1.rds.amazonaws.com/gr5069\",\n",
    "    driver=\"com.mysql.jdbc.Driver\",\n",
    "    dbtable=\"rf_predictions_f1\",\n",
    "    user=\"admin\",\n",
    "    password=\"PYMpym0919!\"\n",
    ").mode(\"overwrite\").save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2eb20ed-da91-4008-b531-cf04710349cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Start MLflow experiment for Linear Regression\n",
    "with mlflow.start_run(run_name=\"LinearRegression_F1\"):\n",
    "\n",
    "    # Train the model\n",
    "    lr = LinearRegression(featuresCol=\"features\", labelCol=\"milliseconds\")\n",
    "    lrModel = lr.fit(trainDF)\n",
    "\n",
    "    # Predict on test set\n",
    "    predDF_lr = lrModel.transform(testDF)\n",
    "\n",
    "    # Evaluate predictions\n",
    "    evaluator = RegressionEvaluator(labelCol=\"milliseconds\", predictionCol=\"prediction\")\n",
    "    rmse = evaluator.setMetricName(\"rmse\").evaluate(predDF_lr)\n",
    "    mae = evaluator.setMetricName(\"mae\").evaluate(predDF_lr)\n",
    "    r2 = evaluator.setMetricName(\"r2\").evaluate(predDF_lr)\n",
    "    mse = evaluator.setMetricName(\"mse\").evaluate(predDF_lr)\n",
    "\n",
    "    # Log parameters (no hyperparameters tuned, just record default info)\n",
    "    mlflow.log_param(\"elasticNetParam\", lr.getElasticNetParam())\n",
    "    mlflow.log_param(\"regParam\", lr.getRegParam())\n",
    "\n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"rmse\", rmse)\n",
    "    mlflow.log_metric(\"mae\", mae)\n",
    "    mlflow.log_metric(\"r2\", r2)\n",
    "    mlflow.log_metric(\"mse\", mse)\n",
    "\n",
    "    # Log model\n",
    "    mlflow.spark.log_model(lrModel, \"model\")\n",
    "\n",
    "    # Log artifacts (e.g., model summary text)\n",
    "    with open(\"/tmp/lr_model_summary.txt\", \"w\") as f:\n",
    "        f.write(str(lrModel.summary))\n",
    "\n",
    "    mlflow.log_artifact(\"/tmp/lr_model_summary.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b56fdbdf-60e0-4ce1-bd31-571b01fe19d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Start MLflow experiment for Random Forest\n",
    "with mlflow.start_run(run_name=\"RandomForest_F1\"):\n",
    "\n",
    "    # Train the Random Forest model\n",
    "    rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"milliseconds\", numTrees=100)\n",
    "    rfModel = rf.fit(trainDF)\n",
    "\n",
    "    # Predict on test set\n",
    "    predDF_rf = rfModel.transform(testDF)\n",
    "\n",
    "    # Evaluate predictions\n",
    "    evaluator = RegressionEvaluator(labelCol=\"milliseconds\", predictionCol=\"prediction\")\n",
    "    rmse = evaluator.setMetricName(\"rmse\").evaluate(predDF_rf)\n",
    "    mae = evaluator.setMetricName(\"mae\").evaluate(predDF_rf)\n",
    "    r2 = evaluator.setMetricName(\"r2\").evaluate(predDF_rf)\n",
    "    mse = evaluator.setMetricName(\"mse\").evaluate(predDF_rf)\n",
    "\n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"numTrees\", rf.getNumTrees())\n",
    "    mlflow.log_param(\"maxDepth\", rf.getMaxDepth())\n",
    "\n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"rmse\", rmse)\n",
    "    mlflow.log_metric(\"mae\", mae)\n",
    "    mlflow.log_metric(\"r2\", r2)\n",
    "    mlflow.log_metric(\"mse\", mse)\n",
    "\n",
    "    # Log model\n",
    "    mlflow.spark.log_model(rfModel, \"model\")\n",
    "\n",
    "    # Log artifact (feature importance)\n",
    "    feature_importances = rfModel.featureImportances\n",
    "    with open(\"/tmp/rf_feature_importances.txt\", \"w\") as f:\n",
    "        f.write(str(feature_importances))\n",
    "\n",
    "    mlflow.log_artifact(\"/tmp/rf_feature_importances.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10fca48f-e079-4a13-99e1-b0db5aa0d011",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "hw5",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
